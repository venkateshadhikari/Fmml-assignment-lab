{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkateshadhikari/Fmml-assignment-lab/blob/main/FMML_ClassificationII_Lab3_GiniImpurity_Generalizability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK5nkETWlnfU"
      },
      "source": [
        "# Lab 3\n",
        "# Classification II : Information Metrics and generalizability of Decision Trees\n",
        "\n",
        "```\n",
        "Module Coordinator : Arohi Srivastav\n",
        "```\n",
        "\n",
        "### RECAP:\n",
        "\n",
        "In the last lab, we learnt:\n",
        "\n",
        "- What is a decision tree and how is a DT used to made a prediction.\n",
        "- How the complexity of tree effects its performance. (An example where increasing the complexity improves the performance.)\n",
        "- Notion of Entropy and Information Gain and how they can be used to create a tree.\n",
        "- Nature of decision boundaries made by DTs.\n",
        "\n",
        "\n",
        "### In this lab:\n",
        "\n",
        "- Introduction to Gini Index and how can it be used to create decision tree.\n",
        "- Curse of Overfitting and how it affects decision trees.\n",
        "- Application of Decision Trees on a real dataset and further experiments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfPd1_dZKU2m"
      },
      "source": [
        "---\n",
        "\n",
        "## Gini Impurity:\n",
        "\n",
        "Gini Impurity is another metric like Entropy which can be used to score a group's homogenity. \n",
        "A group's Gini Impurity is given by:\n",
        "\n",
        "$$ \\text{Gini Index} = 1 - \\sum_{i=1}^{c} p_i^2 $$\n",
        "\n",
        "Whichever division leads to minimum impurity among the divided class is chosen to be the split at that stage.\n",
        "\n",
        "`Impurity of a division =  Weighted sum of the impurity of the subgroups made after the division`\n",
        "\n",
        "Among Gini impurity and Information Gain, there isn't a clear cut better method of developing a decision tree. Though many a times Gini Impurity is the preferred method since it is computationally lesser expensive.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydOB8-b4k5hU",
        "outputId": "b1285d03-d5fa-4b90-8002-857ed151610c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad549076d7d1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menable_iterative_imputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterativeImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.9/dist-packages/sklearn/metrics/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wkpt7Mg5kGe"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1, drawTree = (8,6)):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"The confusion matrix is : \")\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize=(3, 3))\n",
        "  plot_confusion_matrix(clf, X_test, y_test, display_labels=class_names, ax=ax)\n",
        "  plt.show()\n",
        "\n",
        "  if drawTree:\n",
        "    print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "    fig, ax = plt.subplots(figsize=drawTree)\n",
        "    imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 10)\n",
        "    plt.show()\n",
        "\n",
        "def returnAccuracy(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    \n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_pred, y_test)\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "  \n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(5,4))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSsre0KN5C-H"
      },
      "source": [
        "Let us create a synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyH1HwtawwNk"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "ar = np.vstack(     [\\\n",
        "                    np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 50, ),\\\n",
        "                    np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 50, )\n",
        "                    ]\\\n",
        "              )\n",
        "\n",
        "testAr = np.vstack(   [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), np.array([[0.5, -0.25], [-0.25, 0.5]]), size = 500, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 500, )\n",
        "                      ]\\\n",
        "                  )\n",
        "testy = np.array([0] * int((testAr.shape[0]/2)) + [1] * int((testAr.shape[0]/2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evClB9vYxIOY"
      },
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.scatter(ar[:, 0], ar[:, 1], c = np.array([\"r\"] * int((ar.shape[0]/2)) + [\"b\"] * int((ar.shape[0]/2))), )\n",
        "plt.title(\"Noisy Training Data\")\n",
        "plt.xlim((-3, 8))\n",
        "plt.ylim((-3, 8))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.title(\"Real pattern of information\")\n",
        "plt.scatter(testAr[:, 0], testAr[:, 1], c = np.array([\"r\"] * int((testAr.shape[0]/2)) + [\"b\"] * int((testAr.shape[0]/2))), )\n",
        "\n",
        "plt.xlim((-3, 8))\n",
        "plt.ylim((-3, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfXScFLH-loQ"
      },
      "source": [
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "fro, to = 1, 15\n",
        "plt.plot(range(fro, to), [returnAccuracy((X, y), (testAr, testy), max_depth = i) for i in range(fro, to)], \"b.-\", linewidth=0.5)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Depth of the tree\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMekSS9ExeWQ"
      },
      "source": [
        "def boundaryExp(d) :\n",
        "\n",
        "  clf = tree.DecisionTreeClassifier(random_state = 0, max_depth = d)\n",
        "  pair = [0, 1]\n",
        "  clf.fit(X[:, pair], y)\n",
        "  print(\"Depth = {}\".format(d))\n",
        "  plotDecisionBoundary(X, y, pair, clf)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "_ = [boundaryExp(i) for i in [1, 2, 4, 8]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mZJ_nuAQIHV"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "performExperiment((X, y), (testAr, testy), max_depth = 2, drawTree = (8,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJP-iW5hz4D0"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "performExperiment((X, y), (testAr, testy), max_depth = 4, drawTree= (12, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ1Q39yh9ry1"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "performExperiment((X, y), (testAr, testy), max_depth = 8, drawTree = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9hb0t5e2M0M"
      },
      "source": [
        "Now that we have looked at the curse of overfitting and its consequence on the test accuracy, let us try to delve deeper and try to understand that why is it such a serious issue especially for Decision Tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPVQHxyv4zDS"
      },
      "source": [
        "---\n",
        "\n",
        "## Finding Pattern out of nowhere?\n",
        "\n",
        "\n",
        "In the following cell we are generating a series of random numbers from a 2-D uncorrelated Gaussian distribution. And then we randomly assign a class to each of these datapoints. i.e There exists no real pattern in the dataset and we are simply giving them a class arbitrarily.\n",
        "\n",
        "Now let us see how out decision tree does on this when the same set is used as the test set as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGWQcvqr2pAq"
      },
      "source": [
        "SIZE_DECOY = 100\n",
        "np.random.seed(0)\n",
        "simpleDat = np.random.multivariate_normal((0, 0), [[2, 0], [0, 2]], size = SIZE_DECOY)\n",
        "decoyY = np.random.randint(0, 2, size = SIZE_DECOY)\n",
        "COLS = [\"red\", \"blue\"]\n",
        "plt.style.use(\"seaborn\")\n",
        "plt.scatter(simpleDat[:, 0], simpleDat[:, 1], c = [COLS[i] for i in decoyY])\n",
        "plt.xlim((-5, 5)), plt.ylim((-5, 5))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aQ99h6W6KHy"
      },
      "source": [
        "\n",
        "clf = tree.DecisionTreeClassifier(random_state = 0)\n",
        "pair = [0, 1]\n",
        "clf.fit(simpleDat[:, pair], decoyY)\n",
        "plt.style.use(\"default\")\n",
        "plotDecisionBoundary(simpleDat, decoyY, pair, clf)\n",
        "plt.show()\n",
        "\n",
        "performExperiment((simpleDat, decoyY), (simpleDat, decoyY), drawTree=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x2P_RMy7Wg4"
      },
      "source": [
        "Wow, we see that our decision tree is able to predict the values in our train set with $100\\%$  accuracy!\n",
        "\n",
        "That is not a surprise if we pay attention to the nature of Decision Trees. They are in fact capable of **adapting completely to any discrete binary function that exists**. This behavious can be explained using the point they can be seen as nothing but piecewise function definition itself.\n",
        "\n",
        "Does that mean that our decision tree has learnt something? Let us explore by testing it on another set of points taken from the same distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91wSCnLH83hH"
      },
      "source": [
        "SIZE_DECOY = 100\n",
        "np.random.seed(1)\n",
        "testSimpleDat = np.random.multivariate_normal((0, 0), [[2, 0], [0, 2]], size = SIZE_DECOY)\n",
        "testDecoyY = np.random.randint(0, 2, size = SIZE_DECOY)\n",
        "performExperiment((simpleDat, decoyY), (testSimpleDat, testDecoyY), drawTree=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vleeyJ2X-A2m"
      },
      "source": [
        "We infact see that our decision tree has not learnt much since its accuracy is $48\\%$ on a set that it hasn't seen before which is to be expected since there are only 2 classes to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnNK8FOZY-nX"
      },
      "source": [
        "---\n",
        "\n",
        "## Experiment on Titanic Dataset\n",
        "\n",
        "`Reference Dataset taken from: https://www.kaggle.com/c/titanic/data`\n",
        "\n",
        "\n",
        "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
        "\n",
        "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
        "\n",
        "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
        "\n",
        "Our task in rest of today's lab is to use decision trees to predict if a person would be able to survive or not. \n",
        "\n",
        "<table>\n",
        "<tbody>\n",
        "<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n",
        "<tr>\n",
        "<td>survival</td>\n",
        "<td>Survival</td>\n",
        "<td>0 = No, 1 = Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>pclass</td>\n",
        "<td>Ticket class</td>\n",
        "<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sex</td>\n",
        "<td>Sex</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Age</td>\n",
        "<td>Age in years</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sibsp</td>\n",
        "<td># of siblings / spouses aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>parch</td>\n",
        "<td># of parents / children aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ticket</td>\n",
        "<td>Ticket number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>fare</td>\n",
        "<td>Passenger fare</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>cabin</td>\n",
        "<td>Cabin number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>embarked</td>\n",
        "<td>Port of Embarkation</td>\n",
        "<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlIrzXfzV-Si"
      },
      "source": [
        "import os\n",
        "os.system(\"wget https://raw.githubusercontent.com/Foundations-in-Modern-Machine-Learning/course-contents/main/Classification2/data/titanic/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il3ExmG9bSii"
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW1IGKBIbVR9"
      },
      "source": [
        "import pandas as pd\n",
        "trainDf = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU5JigCTjnc8"
      },
      "source": [
        "# Preprocessing:\n",
        "\n",
        "trainDf.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bYnNT0emiHY"
      },
      "source": [
        "trainDf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQhY6OQjrUa"
      },
      "source": [
        "for idx, row in trainDf.iterrows():\n",
        "  if row[\"Sex\"] == \"female\":\n",
        "    trainDf.at[idx, \"Sex\"] = 0\n",
        "  else:\n",
        "    trainDf.at[idx, \"Sex\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juYt8Vbybd7q"
      },
      "source": [
        "# Since we are only exploring, lets make a validation set out of trainDf:\n",
        "\n",
        "trainSet, testSet = train_test_split(trainDf, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJwHF3RtbnUe"
      },
      "source": [
        "def relevantInfo(df):\n",
        "  X = df.iloc[:, [2, 4, 5, 6, 7, 9]]\n",
        "  y = df.iloc[:, 1]\n",
        "\n",
        "  #Preprocessing to handle the missing data using a regressor\n",
        "  imp = IterativeImputer(max_iter = 10, random_state=0)\n",
        "  imp.fit(X)\n",
        "  newDf = imp.transform(X)\n",
        "\n",
        "  return newDf , y.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_fGAEAte4mY"
      },
      "source": [
        "trainX, trainy = relevantInfo(trainSet)\n",
        "testX, testy = relevantInfo(testSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o7P5oIle5q6"
      },
      "source": [
        "performExperiment((trainX, trainy), (testX, testy), max_depth = 3, feature_names=[trainDf.columns[i] for i in [2, 4, 5, 6, 7, 9]], class_names=[\"Died\", \"Survived\"], drawTree=(20, 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joTfTocTWGT-"
      },
      "source": [
        "MAX_DEPTH = 6 #@param {type: \"slider\", min: 2, max: 10}\n",
        "CRITERION =  \"gini\" #@param [\"gini\", \"entropy\"]\n",
        "MIN_SAMPLES_TO_SPLIT = 2 #@param {type:\"slider\", min:2, max:20, step:2}\n",
        "MIN_SAMPLES_IN_LEAF = 1 #@param {type: \"slider\", min: 1, max: 50}\n",
        "\n",
        "plt.style.use(\"default\")\n",
        "performExperiment((trainX, trainy),\\\n",
        "                  (testX, testy),\\\n",
        "                  max_depth = MAX_DEPTH,\\\n",
        "                  feature_names=[trainDf.columns[i] for i in [2, 4, 5, 6, 7, 9]],\\\n",
        "                  class_names=[\"Died\", \"Survived\"],\\\n",
        "                  min_samples_split = MIN_SAMPLES_TO_SPLIT,\\\n",
        "                  min_samples_leaf = MIN_SAMPLES_IN_LEAF,\\\n",
        "                  drawTree=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzbrk2F-sjsd"
      },
      "source": [
        "Let us iterate over all the possible values for depth to find the most optimum depth possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsScfldthNJ9"
      },
      "source": [
        "plt.style.use(\"seaborn\")\n",
        "plt.figure(figsize=(8,6))\n",
        "fro, to = 1, 15\n",
        "plt.plot(range(fro, to), [returnAccuracy((trainX, trainy), (testX, testy), max_depth = i, min_samples_leaf=3, min_samples_split=6, criterion=\"entropy\") for i in range(fro, to)], \"g.-\", linewidth=0.5)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Depth of the tree\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbCQmV3T-21T"
      },
      "source": [
        "---\n",
        " \n",
        "End of Lab 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_ap9f4-4r0"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}