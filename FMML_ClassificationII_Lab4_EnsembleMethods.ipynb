{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkateshadhikari/Fmml-assignment-lab/blob/main/FMML_ClassificationII_Lab4_EnsembleMethods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy0wZUMYhCAC"
      },
      "source": [
        "# Lab 4\n",
        "\n",
        "# Classification II : Ensemble Methods and Random Forests\n",
        "\n",
        "```\n",
        "Module Coordinator : Arohi Srivastav\n",
        "```\n",
        "\n",
        "RECAP:\n",
        "In the last lab, we learnt:\n",
        "\n",
        "- Gini Index.\n",
        "- The curse of overfitting and a way to fix it.\n",
        "\n",
        "In this lab:\n",
        "\n",
        "- Random Forests as a method to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTRd24CNCNj2"
      },
      "source": [
        "![](https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUlo3cvIwkNC"
      },
      "source": [
        "## Understanding the motive and idea behind random forests\n",
        "\n",
        "A critical point for good machine learning models is the ability to generalise well and not simply memorise the training data. Unfortunately, we were witness to the ablity of Decision Trees to perfectly adapt to the training data in the last lab. As we know, this is called *overfitting* and this happens when we have a flexible model (strong learner) that memorizes the training data by fitting it closely.  Such a model is said to have a high variance because the learned parameters will change fastly and considerably with the training data.\n",
        "\n",
        "The **random forest** is a model made up of many decision trees. \n",
        "\n",
        "- When training, each tree in a random foreest learns from a random pample of the data points. This is called as bootstrapping. Same samples can be used in training of multiple decision trees since the sampling is done with replacement.\n",
        "This is a simple yet elegant method to reduce variance of our model and prevent from increasing unneeded bias.\n",
        "\n",
        "- While testing, we make predicitions by averaging the predictions of each decision tree. In case of a classification task, we simply that that category that has the highest number of votes. \n",
        "\n",
        "This entire process is called as **bagging**. (short for Bootstrap Aggregating)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzmLnnK5xTD4",
        "outputId": "5cff62b3-173c-40dc-f273-939238b0f2b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#Importing the necessary packages\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-60a82b34d13f>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menable_iterative_imputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterativeImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.9/dist-packages/sklearn/metrics/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvvvMBV3xe8Q"
      },
      "source": [
        "def performExperiment(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1, drawTree = (8,6)):\n",
        "  #Importing the Decision tree classifier from sklearn:\n",
        "\n",
        "  clf = RandomForestClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    random_state = 0,\\\n",
        "                                    n_estimators = 10\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(\"Accuracy of the Random Forest on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))\n",
        "\n",
        "  print(\"The confusion matrix is : \")\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize=(3, 3))\n",
        "  plot_confusion_matrix(clf, X_test, y_test, display_labels=class_names, ax=ax)\n",
        "  plt.show()\n",
        "\n",
        "  if drawTree:\n",
        "    print(\"Here is a diagram of the tree created to evaluate each sample:\")\n",
        "    fig, ax = plt.subplots(figsize=drawTree)\n",
        "    imgObj = tree.plot_tree(clf, filled=True, ax=ax, feature_names = feature_names, class_names = class_names, impurity=False, proportion=True, rounded=True, fontsize = 10)\n",
        "    plt.show()\n",
        "\n",
        "def returnAccuracy(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  clf = RandomForestClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    random_state = 0,\\\n",
        "                                    n_estimators = 10, \\\n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_pred, y_test)\n",
        "\n",
        "def returnAccuracyDT(trainSet : tuple, testSet : tuple, max_depth : int = None, feature_names : list = None, class_names : list = None, criterion = \"gini\", min_samples_split : int = 2 , min_samples_leaf = 1):\n",
        "  clf = tree.DecisionTreeClassifier(max_depth = max_depth, \\\n",
        "                                    criterion = criterion,\\\n",
        "                                    min_samples_split = min_samples_split,\\\n",
        "                                    min_samples_leaf = min_samples_leaf,\\\n",
        "                                    splitter = \"best\",\\\n",
        "                                    random_state = 0,\\\n",
        "                                    \n",
        "                                    )\n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf = clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_pred, y_test)\n",
        "\n",
        "def returnAccuracySvm(trainSet : tuple, testSet : tuple):\n",
        "  clf = svm.LinearSVC()\n",
        "                                    \n",
        "  X_train, y_train = trainSet\n",
        "  X_test, y_test = testSet\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  return accuracy_score(y_pred, y_test)\n",
        "\n",
        "def giveAnExample(n : int):\n",
        "  performExperiment((X_train, y_train),  (X_test, y_test), feature_names = iris[\"feature_names\"], class_names = iris[\"target_names\"], max_depth = n)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "  \n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(5,4))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  # plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOGrJtt8xkux"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "ar = np.vstack(     [\\\n",
        "                    np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 50, ),\\\n",
        "                    np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 50, )\n",
        "                    ]\\\n",
        "              )\n",
        "\n",
        "testAr = np.vstack(   [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), np.array([[0.5, -0.25], [-0.25, 0.5]]), size = 500, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 500, )\n",
        "                      ]\\\n",
        "                  )\n",
        "testy = np.array([0] * int((testAr.shape[0]/2)) + [1] * int((testAr.shape[0]/2)))\n",
        "\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thtYDz2YxsGl"
      },
      "source": [
        "plt.figure(figsize=(5,4))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.scatter(ar[:, 0], ar[:, 1], c = np.array([\"r\"] * int((ar.shape[0]/2)) + [\"b\"] * int((ar.shape[0]/2))), )\n",
        "plt.title(\"Noisy Training Data\")\n",
        "plt.xlim((-3, 8))\n",
        "plt.ylim((-3, 8))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.title(\"Real pattern of information\")\n",
        "plt.scatter(testAr[:, 0], testAr[:, 1], c = np.array([\"r\"] * int((testAr.shape[0]/2)) + [\"b\"] * int((testAr.shape[0]/2))), )\n",
        "\n",
        "plt.xlim((-3, 8))\n",
        "plt.ylim((-3, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzdH85RvykXF"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "\n",
        "def boundaryExp(d) :\n",
        "  clf = RandomForestClassifier(random_state = 0, max_depth = d)\n",
        "  pair = [0, 1]\n",
        "  clf.fit(X[:, pair], y)\n",
        "  print(\"Depth = {}\".format(d))\n",
        "  plotDecisionBoundary(X, y, pair, clf)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "_ = [boundaryExp(i) for i in [1, 2, 4, 8]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RW1w1YKPumaM"
      },
      "source": [
        "plt.style.use(\"seaborn\")\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "fro, to = 1, 15\n",
        "plt.plot(range(fro, to), [returnAccuracy((X, y), (testAr, testy), max_depth = i) for i in range(fro, to)], \"b.-\", linewidth=0.5)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Depth of the tree\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ9bcNnchgvY"
      },
      "source": [
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "\n",
        "def boundaryExp() :\n",
        "  clf = svm.LinearSVC()\n",
        "  pair = [0, 1]\n",
        "  clf.fit(X[:, pair], y)\n",
        "  plotDecisionBoundary(X, y, pair, clf)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "boundaryExp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yt95KcqichO"
      },
      "source": [
        "\n",
        "returnAccuracySvm((X, y), (testAr, testy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHaz3NhQzzen"
      },
      "source": [
        "---\n",
        "\n",
        "## Experiment on Titanic Dataset\n",
        "\n",
        "`Reference Dataset taken from: https://www.kaggle.com/c/titanic/data`\n",
        "\n",
        "\n",
        "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
        "\n",
        "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
        "\n",
        "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
        "\n",
        "Our task in rest of today's lab is to use decision trees to predict if a person would be able to survive or not. \n",
        "\n",
        "<table>\n",
        "<tbody>\n",
        "<tr><th><b>Variable</b></th><th><b>Definition</b></th><th><b>Key</b></th></tr>\n",
        "<tr>\n",
        "<td>survival</td>\n",
        "<td>Survival</td>\n",
        "<td>0 = No, 1 = Yes</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>pclass</td>\n",
        "<td>Ticket class</td>\n",
        "<td>1 = 1st, 2 = 2nd, 3 = 3rd</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sex</td>\n",
        "<td>Sex</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>Age</td>\n",
        "<td>Age in years</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>sibsp</td>\n",
        "<td># of siblings / spouses aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>parch</td>\n",
        "<td># of parents / children aboard the Titanic</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>ticket</td>\n",
        "<td>Ticket number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>fare</td>\n",
        "<td>Passenger fare</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>cabin</td>\n",
        "<td>Cabin number</td>\n",
        "<td></td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>embarked</td>\n",
        "<td>Port of Embarkation</td>\n",
        "<td>C = Cherbourg, Q = Queenstown, S = Southampton</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TasV3ae-z0So"
      },
      "source": [
        "import os\n",
        "os.system(\"wget https://raw.githubusercontent.com/Foundations-in-Modern-Machine-Learning/course-contents/main/Classification2/data/titanic/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyM43yHM0irZ"
      },
      "source": [
        "import pandas as pd\n",
        "trainDf = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbT5KHlX0xJ7"
      },
      "source": [
        "for idx, row in trainDf.iterrows():\n",
        "  if row[\"Sex\"] == \"female\":\n",
        "    trainDf.at[idx, \"Sex\"] = 0\n",
        "  else:\n",
        "    trainDf.at[idx, \"Sex\"] = 1\n",
        "    # Since we are only exploring, lets make a validation set out of trainDf:\n",
        "\n",
        "trainSet, testSet = train_test_split(trainDf, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzdjEFxt02pJ"
      },
      "source": [
        "def relevantInfo(df):\n",
        "  X = df.iloc[:, [2, 4, 5, 6, 7, 9]]\n",
        "  y = df.iloc[:, 1]\n",
        "\n",
        "  #Preprocessing to handle the missing data using a regressor\n",
        "  imp = IterativeImputer(max_iter = 10, random_state=0)\n",
        "  imp.fit(X)\n",
        "  newDf = imp.transform(X)\n",
        "\n",
        "  return newDf , y.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM0PG2Wx04mz"
      },
      "source": [
        "trainX, trainy = relevantInfo(trainSet)\n",
        "testX, testy = relevantInfo(testSet)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-DDxxvr0_pk"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "performExperiment((trainX, trainy), (testX, testy), max_depth = 6, feature_names=[trainDf.columns[i] for i in [2, 4, 5, 6, 7, 9]], class_names=[\"Died\", \"Survived\"], drawTree=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yqUPCAd1AP_"
      },
      "source": [
        "plt.style.use(\"seaborn\")\n",
        "plt.figure(figsize=(8,6))\n",
        "fro, to = 1, 15\n",
        "plt.plot(range(fro, to), [returnAccuracy((trainX, trainy), (testX, testy), max_depth = i) for i in range(fro, to)], \"g.-\", linewidth=0.5)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Depth of the tree\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mtC5nrdCTmF"
      },
      "source": [
        "### \"Bagging of Strong Unstable Learning leads to a Strong Stable Learner.\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJSP1Pd41zSy"
      },
      "source": [
        "np.random.seed(0)\n",
        "testAr = np.vstack(   [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), np.array([[0.5, -0.25], [-0.25, 0.5]]), size = 500, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 500, )\n",
        "                      ]\\\n",
        "                  )\n",
        "testy = np.array([0] * int((testAr.shape[0]/2)) + [1] * int((testAr.shape[0]/2)))\n",
        "\n",
        "def stabilityTest():\n",
        "\n",
        "  ar = np.vstack(     [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 30, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 30, )\n",
        "                      ]\\\n",
        "                )\n",
        "\n",
        "  X = ar\n",
        "  y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))  \n",
        "\n",
        "  treeAcc = returnAccuracyDT((X, y), (testAr, testy), max_depth = 4)\n",
        "  forestAcc = returnAccuracy((X, y), (testAr, testy), max_depth = 4)\n",
        "\n",
        "  return treeAcc,  forestAcc\n",
        "\n",
        "treeDat = []\n",
        "forestDat = []\n",
        "for i in range(50):\n",
        "  treeAcc, forestAcc = stabilityTest()\n",
        "  treeDat.append(treeAcc)\n",
        "  forestDat.append(forestAcc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBqMbCxf6FEf"
      },
      "source": [
        "plt.plot(treeDat, \"rv--\", label=\"Decision Trees\")\n",
        "plt.plot(forestDat, \"g^-\", label=\"Random Forests\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOc8KQ0z83l-"
      },
      "source": [
        "# Comparing Speed of training of SVMs and Random Forests\n",
        "\n",
        "Even though SVM's decision boundaries proved to generalize better, we, still have significant motivation to use Ensemble methods like RandomForests over SVMs for the reasons which we will see below:\n",
        "\n",
        "Lets train a set with 20000 datapoints and train two models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS5anWGN6OdP"
      },
      "source": [
        "ar = np.vstack(     [\\\n",
        "                    np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 10000, ),\\\n",
        "                    np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 10000, )\n",
        "                    ]\\\n",
        "              )\n",
        "\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1pRA3Kw-Foa"
      },
      "source": [
        "import time\n",
        "\n",
        "clf = svm.SVC()\n",
        "startTime = time.time()\n",
        "clf.fit(X, y)\n",
        "endTime = time.time()\n",
        "\n",
        "print(\"Time taken for SVM: {}\".format(endTime-startTime))\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10)\n",
        "startTime = time.time()\n",
        "clf.fit(X, y)\n",
        "endTime = time.time()\n",
        "\n",
        "print(\"Time taken for Random Forests: {}\".format(endTime-startTime))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOjSdyw2ANVt"
      },
      "source": [
        "---\n",
        "\n",
        "End of Lab 4"
      ]
    }
  ]
}